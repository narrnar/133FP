---
title: "STATS 133 Final Project"
author: "Daren Sathasivam"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Derek - webscrape
##  Tyler, The Creator
```{r}
library(rvest)
library(stringr)
library(httr)
library(dplyr)

# Set artist name and Genius artist page URL
artist_name <- "Tyler, The Creator"
artist_url <- "https://genius.com/artists/Tyler-the-creator"

# Create main artist folder
dir.create(artist_name, showWarnings = FALSE)

# Scrape album URLs from the artist page
artist_page <- tryCatch(read_html(artist_url), error = function(e) NULL)

if (!is.null(artist_page)) {
  album_urls <- artist_page %>%
    html_nodes("a[href*='/albums/Tyler-the-creator/']") %>%
    html_attr("href") %>%
    unique()
  
  # Loop through each album
  for (album_url in album_urls) {
    Sys.sleep(2)  # Delay to prevent rate-limiting
    
    # Extract album name
    album_name <- str_extract(album_url, "/albums/Tyler-the-creator/[^/]+") %>%
      str_replace("/albums/Tyler-the-creator/", "") %>%
      str_replace_all("-", " ") %>%
      str_to_title()
    
    # Create folder for the album
    album_folder <- file.path(artist_name, album_name)
    dir.create(album_folder, showWarnings = FALSE)
    
    # Try to read the album page
    album_page <- tryCatch(read_html(album_url), error = function(e) NULL)
    
    if (!is.null(album_page)) {
      song_urls <- album_page %>%
        html_nodes("a[href*='/Tyler-the-creator-']") %>%
        html_attr("href") %>%
        unique()
      
      # Loop through each song and save lyrics
      for (song_url in song_urls) {
        Sys.sleep(2)  # Delay between requests
        
        # Extract song name
        song_name <- str_extract(song_url, "Tyler-the-creator-[^/]+") %>%
          str_replace("Tyler-the-creator-", "") %>%
          str_replace_all("-", " ") %>%
          str_to_title() %>%
          paste0(".txt")
        
        # Try to read the song page
        song_page <- tryCatch(read_html(song_url), error = function(e) NULL)
        
        if (!is.null(song_page)) {
          # Extract lyrics
          lyrics <- song_page %>%
            html_nodes("div[data-lyrics-container='true']") %>%
            html_text() %>%
            str_squish()
          
          if (length(lyrics) > 0) {  # Check if lyrics were found
            file_path <- file.path(album_folder, song_name)
            
            # Try to write the file
            tryCatch({
              writeLines(lyrics, file_path)
              print(paste("Saved:", file_path))
            }, error = function(e) {
              print(paste("Error writing file:", file_path))
            })
          } else {
            print(paste("No lyrics found for:", song_name))
          }
        } else {
          print(paste("Error loading song page:", song_url))
        }
      }
    } else {
      print(paste("Error loading album page:", album_url))
    }
  }
} else {
  print("Error loading artist page.")
}
```

# Derek - webscrape
## MF DOOM
```{r}
library(rvest)
library(stringr)
library(httr)
library(dplyr)

# Set artist name and Genius artist page URL
artist_name <- "MF DOOM"
artist_url <- "https://genius.com/artists/Mf-doom"

# Create main artist folder
dir.create(artist_name, showWarnings = FALSE)

# Scrape album URLs from the artist page
artist_page <- tryCatch(read_html(artist_url), error = function(e) NULL)

if (!is.null(artist_page)) {
  album_urls <- artist_page %>%
    html_nodes("a[href*='/albums/Mf-doom/']") %>%
    html_attr("href") %>%
    unique()
  
  # Loop through each album
  for (album_url in album_urls) {
    Sys.sleep(2)  # Delay to prevent rate-limiting
    
    # Extract album name
    album_name <- str_extract(album_url, "/albums/Mf-doom/[^/]+") %>%
      str_replace("/albums/Mf-doom/", "") %>%
      str_replace_all("-", " ") %>%
      str_to_title()
    
    # Create folder for the album
    album_folder <- file.path(artist_name, album_name)
    dir.create(album_folder, showWarnings = FALSE)
    
    # Try to read the album page
    album_page <- tryCatch(read_html(album_url), error = function(e) NULL)
    
    if (!is.null(album_page)) {
      song_urls <- album_page %>%
        html_nodes("a[href*='/Mf-doom-']") %>%
        html_attr("href") %>%
        unique()
      
      # Loop through each song and save lyrics
      for (song_url in song_urls) {
        Sys.sleep(2)  # Delay between requests
        
        # Extract song name
        song_name <- str_extract(song_url, "Mf-doom-[^/]+") %>%
          str_replace("Mf-doom-", "") %>%
          str_replace_all("-", " ") %>%
          str_to_title() %>%
          paste0(".txt")
        
        # Try to read the song page
        song_page <- tryCatch(read_html(song_url), error = function(e) NULL)
        
        if (!is.null(song_page)) {
          # Extract lyrics
          lyrics <- song_page %>%
            html_nodes("div[data-lyrics-container='true']") %>%
            html_text() %>%
            str_squish()
          
          if (length(lyrics) > 0) {  # Check if lyrics were found
            file_path <- file.path(album_folder, song_name)
            
            # Try to write the file
            tryCatch({
              writeLines(lyrics, file_path)
              print(paste("Saved:", file_path))
            }, error = function(e) {
              print(paste("Error writing file:", file_path))
            })
          } else {
            print(paste("No lyrics found for:", song_name))
          }
        } else {
          print(paste("Error loading song page:", song_url))
        }
      }
    } else {
      print(paste("Error loading album page:", album_url))
    }
  }
} else {
  print("Error loading artist page.")
}
```



# Extract artist to lyric txt w/ geniusr

```{r}
library(geniusr)
library(dplyr)
library(purrr)
library(stringr)
library(fs)

# Sys.setenv(GENIUS_API_TOKEN = "yxwe42jdT0MvORWPgZ4HdKyNx9axVEQWPCvjkSQ0zjGNUYZ2mGIqBrAuW9CpUAWG")

# --- Client ID --- #
# naXHkMhBTywrOh0ova7Pmh_7tTOcFJ1EpQfKTNvZlyM4wsRPZL7m_--NCwt6an32

# --- Client Secret --- #
# c_1ut-Bgqy7agNISeCTTET6A68C5hxiFiBqB7UFGXQDE4pgmcDewv4pUHIn3cbVWWMHua3lKrh2YLJRtNmxu7Q

# --- Client Access Token --- #
# yxwe42jdT0MvORWPgZ4HdKyNx9axVEQWPCvjkSQ0zjGNUYZ2mGIqBrAuW9CpUAWG
# genius_token(yxwe42jdT0MvORWPgZ4HdKyNx9axVEQWPCvjkSQ0zjGNUYZ2mGIqBrAuW9CpUAWG)
# 
# # Check if token is working:
# search_genius("MF DOOM")
# search_artist("MF DOOM")

scrape_artist_discography <- function(artist_name, artist_num) {
  # Setup directories --- paste ur directory below
  output_dir <- path(
    "/Users/darensivam/Desktop/UCLA/Year 4/Winter/133/Final Project/lyrics", 
    paste0("artist", artist_num)
  )
  dir_create(output_dir)
  
  # 1. Retrieve all possible matches for the given artist_name
  artist_df <- search_artist(artist_name) %>%
    slice(1)  # just take the top row
  # --- Debug: If no match then artist not found --- #
  if (nrow(artist_df) == 0) {
    stop("Artist not found: ", artist_name)
  }
  
  # 2. Get songs with the found artist ID
  songs <- get_artist_songs_df(
    artist_id = artist_df$artist_id,
    include_features = FALSE
  ) %>%
    distinct(song_id, .keep_all = TRUE)

  # --- Debug: If no songs found, you can handle it gracefully --- #
  if (nrow(songs) == 0) {
    message("No songs found for: ", artist_name)
    return(invisible(NULL))
  }
  
  # 3. Group songs by album
  songs %>%
    group_by(
      album_id, 
      album_name = coalesce(album_name, "Non-Album") # stackechange
    ) %>%
    arrange(track_number, .by_group = TRUE) %>%
    group_walk(~ { # stackexhange help
      safe_album_name <- .y$album_name %>%
        str_remove_all("[^\\w ]") %>%
        str_squish()
      # Store album
      album_dir <- path(output_dir, safe_album_name)
      dir_create(album_dir)
      
      # 4. For each song, get lyrics and write to file
      walk2(.x$song_id, seq_along(.x$song_id), ~ {
        song_title <- .x$title %>%
          str_remove_all("[^\\w ]") %>%
          str_squish()
        
        song_file <- path(album_dir, sprintf("%02d_%s.txt", .y, song_title))
        # --- Debug: if song file already exists --- #
        if (!file_exists(song_file)) {
          try({
            lyrics <- get_lyrics_id(.x$song_id)$line %>%
              paste(collapse = "\n") %>%
              str_remove_all("\\[[^\\]]+\\]")  # optional bracket removal
            writeLines(lyrics, song_file)
            Sys.sleep(0.8)  # courtesy delay recommended in 102a by Prof Chen
          }, silent = TRUE)
        }
      })
    })
}


# --- USAGE --- #
# 1) MF DOOM
scrape_artist_discography("MF DOOM", 1)
# 2) Tyler, The Creator
scrape_artist_discography("Tyler, the Creator", 2)
```


# Ethan - Extract txt from discography -- more code to fix censoring and resulting plots

```{r}
library(dplyr)
library(stringr)
library(tidytext)
library(readtext)
library(tidyr)
library(quanteda)
library(tm)

censor_words <- function(text) {
  # text <- gsub("\\b(n|N)(i|I)(g|G)(g|G)(a|A)\\b", "\\1\\2**a", text, perl = TRUE)
  # text <- gsub("\\b(n|N)(i|I)(g|G)(g|G)(a|A)\\b", "\\1\\2**as", text, perl = TRUE)
  # text <- gsub("\\b(n|N)(i|I)(g|G)(g|G)(e|E)(r|R)\\b", "\\1\\2**er", text, perl = TRUE)
  text <- gsub("\\b([nN])([iI])([gG])([gG])((?:a|er|as|ers)?)\\b", "\\1\\2**\\5", text, perl = TRUE)
  text <- gsub("\\b(f|F)(a|A)(g|G)(g|G)(o|O)(t|T)\\b", "\\1\\2**ot", text, perl = TRUE)
  # text <- gsub("\\b(n|N)(i|I)(g|G)(g|G)(\\w*)\\b", "\\1\\2**\\5", text, perl = TRUE)
  # text <- gsub("\\b(f|F)(a|A)(g|G)(g|G)(\\w*)\\b", "\\1\\2**\\5", text, perl = TRUE)
  
  # text <- gsub("\\b([fF]([uU]([cC])([kK])\\b", "\\1\\2**\\3")
  return(text)
}

remove_custom_punctuation <- content_transformer(function(text) {
  # First, replace curly quotes with a standard ASCII version
  text <- str_replace_all(text, "[‘’´`]+", "'")
  text <- str_replace_all(text, '[“”]+', '"')
  # remove punctuation except '*'
  text <- gsub("[[:punct:]&&[^*]]", "", text)
  text
})


data("stop_words")

Tyler_albums <- readtext("/Users/darensivam/Desktop/UCLA/Year 4/Winter/133/Final Project/lyrics/Tyler, The Creator")
Tyler_combined <- paste(Tyler_albums$text, collapse = " ")

# See what needs to be filtered out
# Tyler_combined

# Some of the sentences include who is speaking in brackets, so we need to filter that out
Tyler_combined_clean <- gsub("\\[.*?\\]", "", Tyler_combined)

# Some of the words did not get spaced properly between verses, ex. arriveIn, so we need to add space for that
Tyler_combined_clean <- gsub("([A-Z])", " \\1", Tyler_combined_clean)

# Random /n in the string, needed to be removed
Tyler_combined_clean <- gsub("\n", " ", Tyler_combined_clean)


Tyler_corp <- Corpus(VectorSource(Tyler_combined_clean))

Tyler_corp <- tm_map(Tyler_corp, content_transformer(tolower)) 
Tyler_corp <- tm_map(Tyler_corp, removePunctuation)  
Tyler_corp <- tm_map(Tyler_corp, removeNumbers) 
Tyler_corp <- tm_map(Tyler_corp, stripWhitespace)
Tyler_corp <- tm_map(Tyler_corp, removeWords, stopwords("en"))

Tyler_corp <- censor_words(Tyler_corp)
Tyler_combined_clean <- censor_words(Tyler_combined_clean)


# inspect(Tyler_corp)

# Inspection looks good, making DTM
DocumentTermMatrix(Tyler_corp)

Tyler_t <- tokens(corpus(Tyler_corp))

# ---------------------------------------------------------------------------------------------------------------


Doom_albums <- readtext("/Users/darensivam/Desktop/UCLA/Year 4/Winter/133/Final Project/lyrics/MF DOOM")
Doom_combined <- paste(Doom_albums$text, collapse = " ")

# Doom_combined

Doom_combined_clean <- gsub("\\[.*?\\]", "", Doom_combined)
Doom_combined_clean <- gsub("([A-Z])", " \\1", Doom_combined_clean)
Doom_combined_clean <- gsub("\n", " ", Doom_combined_clean)


Doom_corp <- Corpus(VectorSource(Doom_combined_clean))

Doom_corp <- tm_map(Doom_corp, content_transformer(tolower)) 
Doom_corp <- tm_map(Doom_corp, removePunctuation)  
Doom_corp <- tm_map(Doom_corp, removeNumbers) 
Doom_corp <- tm_map(Doom_corp, stripWhitespace)
Doom_corp <- tm_map(Doom_corp, removeWords, stopwords("en"))

Doom_corp <- censor_words(Doom_corp)
Doom_combined_clean <- censor_words(Doom_combined_clean)

# inspect(Doom_corp)

# Inspection looks good, making DTM
DocumentTermMatrix(Doom_corp)

Doom_t <- tokens(corpus(Doom_corp))
```

# Outline of things to do:

- 1. Word Analysis: Sentiment Analysis, Word Clouds
- 2. N-grams: Bi-grams by counts, Negation Analysis
- 3. Topic Modeling: LDA, Topic Distribution by Source, then Random Forest


#  Data cleaned -> proceed with text mining methods/steps

```{r}
library(ggplot2)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
library(widyr)
# 1. Clean data for specific terms and expressions
# head(Doom_combined_clean, 1)
# head(Tyler_combined_clean, 1)


# 2. String distances
library(stringdist)
tyler_lines <- unlist(strsplit(Tyler_combined_clean, "\\.\\s+"))
doom_lines <- unlist(strsplit(Doom_combined_clean, "\\.\\s+"))
dist_mat <- stringdistmatrix(tyler_lines, doom_lines, method = "jw") # jw for Jaro-Wrinkler dists --- stackexhange ---
# dist_mat


# 3. Graphical displays of text data
# Word freq comparison plot
word_freq_df <- data.frame(word = c(names(topfeatures(dfm(Tyler_t), 10)), names(topfeatures(dfm(Doom_t), 10))),
                           freq = c(topfeatures(dfm(Tyler_t), 10), topfeatures(dfm(Doom_t), 10)),
                           artist = rep(c("Tyler", "Doom"), each = 10))
ggplot(word_freq_df, aes(x = reorder(word, freq), y = freq, fill = artist)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~artist, scales = "free") +
  labs(title = "Top 10 Words Comparison")
# Word Clouds -- basic for each artist
tyler_dtm <- DocumentTermMatrix(Tyler_corp)
tyler_matrix <- as.matrix(tyler_dtm)
tyler_word_freq <- sort(colSums(tyler_matrix), decreasing = TRUE)
set.seed(1)
wordcloud(words = names(tyler_word_freq), freq = tyler_word_freq, min.freq = 5, max.words = 60, scale = c(4, 0.75), colors = brewer.pal(8, "Dark2"))
doom_dtm <- DocumentTermMatrix(Doom_corp)
doom_matrix <- as.matrix(doom_dtm)
doom_word_freq <- sort(colSums(doom_matrix), decreasing = TRUE)
set.seed(1)
wordcloud(words = names(doom_word_freq), freq = doom_word_freq, min.freq = 5, max.words = 60, scale = c(4, 0.75), colors = brewer.pal(8, "Dark2"))


# 4. Natural language processing: stemming, parts-of-speech tagging
# Stemming
# tyler_stemmed <- tm_map(Tyler_corp, stemDocument)
# doom_stemmed <- tm_map(Doom_corp, stemDocument)
# Parts-of-speech tagging
library(udpipe) # stackechange
ud_model <- udpipe_download_model(language = "english")
tyler_pos <- udpipe(Tyler_combined_clean, "english")
doom_pos <- udpipe(Doom_combined_clean, "english")


# 5. Tokenization and lemmatization
# Tokens w/ lemmatization -- different from Tyler_t and Doom_t
tyler_tokens <- tyler_pos %>%
  filter(upos %in% c("NOUN", "VERB", "ADJ")) %>%
  select(token, lemma)
doom_tokens <- doom_pos %>%
  filter(upos %in% c("NOUN", "VERB", "ADJ")) %>%
  select(token, lemma)

# 6. Creating Corpus - done above

# 7. Creating Document term matrix - done above

# 8. Analyzing Word and Document Frequency: TF-idf
tyler_tfidf <- weightTfIdf(DocumentTermMatrix(Tyler_corp))
findMostFreqTerms(tyler_tfidf, n = 15)
doom_tfidf <- weightTfIdf(DocumentTermMatrix(Doom_corp))
findMostFreqTerms(doom_tfidf, n = 15)

# 9. Relationships Btween Words: N-grams and Correlations
# Bigrams
tyler_bigrams <- Tyler_albums %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE)
doom_bigrams <- Doom_albums %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE)
# Word correlations
tyler_tidy <- tidy(DocumentTermMatrix(Tyler_corp))
tyler_word_cor <- tyler_tidy %>%
  pairwise_cor(term, document, sort = TRUE)
doom_tidy <- tidy(DocumentTermMatrix(Doom_corp))
doom_word_cor <- doom_tidy %>%
  pairwise_cor(term, document, sort = TRUE)
# Correlation plots
tyler_dtm_sparse <- removeSparseTerms(tyler_dtm, 0.90)
doom_dtm_sparse <- removeSparseTerms(doom_dtm, 0.90)
tyler_mat_sparse <- as.matrix(tyler_dtm_sparse)
doom_mat_sparse <- as.matrix(doom_dtm_sparse)
dim(tyler_mat_sparse)
dim(doom_mat_sparse)
tyler_sparse_freq <- colSums(tyler_mat_sparse)
doom_sparse_freq <- colSums(doom_mat_sparse)
tyler_sparse_freq <- sort(tyler_sparse_freq, decreasing = TRUE)
doom_sparse_freq <- sort(doom_sparse_freq, decreasing = TRUE)
head(tyler_sparse_freq, 15)
head(doom_sparse_freq, 15)
tyler_sparse_terms <- findFreqTerms(tyler_dtm_sparse, lowfreq = 500)
doom_sparse_terms <- findFreqTerms(doom_dtm_sparse, lowfreq = 125)
# Since corp is under one doc, the corr plot only turns out to be vertical bars because there are no multiple documents.
plot(tyler_dtm_sparse, terms = tyler_sparse_terms, corThreshold = 0.1)
plot(doom_dtm_sparse, terms = doom_sparse_terms, corThreshold = 0.1)
```


```{r}
##### ----- Other stuff ----- #####
library(tokenizers)
# Readability index and average words per sentence
Tyler_qcorp <- corpus(Tyler_combined_clean)
Doom_qcorp <- corpus(Doom_combined_clean)
tyler_readability <- textstat_readability(Tyler_qcorp, measure = c("Flesch", "Flesch.Kincaid"))
doom_readability  <- textstat_readability(Doom_qcorp, measure = c("Flesch", "Flesch.Kincaid"))
tyler_sentences <- tokenize_sentences(Tyler_combined_clean)[[1]]
doom_sentences <- tokenize_sentences(Doom_combined_clean)[[1]]
yler_words_per_sent <- sapply(tokenize_words(tyler_sentences), length)
doom_words_per_sent <- sapply(tokenize_words(doom_sentences), length)
tyler_avg_words <- mean(tyler_words_per_sent[tyler_words_per_sent > 0])
doom_avg_words <- mean(doom_words_per_sent[doom_words_per_sent > 0])

cat("Tyler Readability:\n")
print(tyler_readability)
cat("\nDOOM Readability:\n")
print(doom_readability)
cat("\nAverage Words Per Sentence:\n")
cat("Tyler:", round(tyler_avg_words, 1), "\n")
cat("DOOM:", round(doom_avg_words, 1), "\n")
# Word counts for each artist
tyler_word_count <- ntoken(tyler_qcorp, remove_punct = TRUE)
doom_word_count <- ntoken(doom_qcorp, remove_punct = TRUE)
cat("\nTotal Word Counts:\n")
cat("Tyler:", tyler_word_count, "\n")
cat("DOOM:", doom_word_count, "\n")
# Plot together:
readability_df <- data.frame(
  Artist = c("Tyler", "DOOM"),
  Flesch = c(tyler_readability$Flesch, doom_readability$Flesch),
  FK_Grade = c(tyler_readability$Flesch.Kincaid, doom_readability$Flesch.Kincaid),
  Avg_Words_Per_Sent = c(tyler_avg_words, doom_avg_words),
  Total_Words = c(tyler_word_count, doom_word_count)
)
ggplot(readability_df, aes(x = Artist, y = FK_Grade, fill = Artist)) +
  geom_col() +
  labs(title = "Readability Comparison", y = "Flesch-Kincaid Grade Level")



# Bigrams for each artist
tyler_df <- tibble(text = Tyler_combined_clean)
tyler_bigrams <- tyler_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !is.na(word1)) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE) %>%
  top_n(15, n)
doom_df <- tibble(text = Doom_combined_clean)
doom_bigrams <- doom_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !is.na(word1)) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE) %>%
  top_n(15, n)
# Combined bigram
combined_bigrams <- bind_rows(
  mutate(tyler_bigrams, artist = "Tyler, The Creator"),
  mutate(doom_bigrams, artist = "MF DOOM")
) %>%
  group_by(bigram) %>%
  summarise(total = sum(n)) %>%
  arrange(desc(total)) %>%
  top_n(15, total)
# Bigram Visualizations
ggplot(tyler_bigrams, aes(x = reorder(bigram, n), y = n)) +
  geom_col(fill = "goldenrod") +
  coord_flip() +
  labs(title = "Top Bigrams in Tyler, The Creator's Lyrics",
       x = "Bigram", y = "Frequency") +
  theme_minimal()
ggplot(doom_bigrams, aes(x = reorder(bigram, n), y = n)) +
  geom_col(fill = "lightblue") +
  coord_flip() +
  labs(title = "Top Bigrams in MF DOOM's Lyrics",
       x = "Bigram", y = "Frequency") +
  theme_minimal()
ggplot(combined_bigrams, aes(x = reorder(bigram, total), y = total)) +
  geom_col(fill = "salmon") +
  coord_flip() +
  labs(title = "Most Frequent Bigrams Across Both Artists",
       x = "Bigram", y = "Total Frequency") +
  theme_minimal()

# Negation Analyis
negation_words <- c("not", "no", "never", "none", "nobody", 
                    "nothing", "n't", "cannot", "without")
# --- Tyler Negation --- #
tyler_bigrams <- tyler_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ", remove = FALSE) %>%
  filter(!is.na(word1), !is.na(word2)) %>%
  mutate(is_negation = word1 %in% negation_words)
tyler_negation_bigrams <- tyler_bigrams %>%
  filter(is_negation)
# Negation with bing sentiment
tyler_negation_sentiment <- tyler_negation_bigrams %>%
  inner_join(get_sentiments("bing"), by = c("word2" = "word"))
tyler_negation_sentiment_flipped <- tyler_negation_sentiment %>%
  mutate(flipped = ifelse(sentiment == "positive", "negative", "positive"))
tyler_negation_summary <- tyler_negation_sentiment_flipped %>%
  count(word2, sentiment, flipped, sort = TRUE)
# tyler_negation_summary
ggplot(tyler_negation_summary, aes(x = flipped, y = n, fill = flipped)) +
  geom_col(show.legend = FALSE) +
  labs(
    title = "Number of Words Flipped by Negation (Tyler)",
    x = NULL, 
    y = "Count of Negation Bigrams"
  ) +
  theme_minimal()
most_negated_words <- tyler_negation_sentiment_flipped %>%
  count(word2, sentiment, flipped, sort = TRUE) %>%
  slice_max(n, n = 15)  # top 15 negated words
ggplot(most_negated_words, aes(x = reorder(word2, n), y = n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ flipped, scales = "free") +
  labs(
    title = "Most Common Negated Words (Tyler)",
    x = "Word That Was Negated", 
    y = "Count"
  ) +
  theme_minimal()
# --- DOOM Negation --- #
doom_bigrams <- doom_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ", remove = FALSE) %>%
  filter(!is.na(word1), !is.na(word2)) %>%
  mutate(is_negation = word1 %in% negation_words)
doom_negation_bigrams <- doom_bigrams %>%
  filter(is_negation)
# Negation with bing sentiment
doom_negation_sentiment <- doom_negation_bigrams %>%
  inner_join(get_sentiments("bing"), by = c("word2" = "word"))
doom_negation_sentiment_flipped <- doom_negation_sentiment %>%
  mutate(flipped = ifelse(sentiment == "positive", "negative", "positive"))
doom_negation_summary <- doom_negation_sentiment_flipped %>%
  count(word2, sentiment, flipped, sort = TRUE)
# tyler_negation_summary
ggplot(doom_negation_summary, aes(x = flipped, y = n, fill = flipped)) +
  geom_col(show.legend = FALSE) +
  labs(
    title = "Number of Words Flipped by Negation (MF DOOM)",
    x = NULL, 
    y = "Count of Negation Bigrams"
  ) +
  theme_minimal()
most_negated_words <- doom_negation_sentiment_flipped %>%
  count(word2, sentiment, flipped, sort = TRUE) %>%
  slice_max(n, n = 15)  # top 15 negated words
ggplot(most_negated_words, aes(x = reorder(word2, n), y = n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ flipped, scales = "free") +
  labs(
    title = "Most Common Negated Words (MF DOOM)",
    x = "Word That Was Negated", 
    y = "Count"
  ) +
  theme_minimal()
```


# Sentiment Analysis

```{r}
# Use lexicons such as afinn and bing
# Sentiments using ncr
# Positive and negative sentiments for each artist
# Other sentiment trends (9 emotions)
# Word clouds
library(reshape2)

tyler_df <- tibble(text = Tyler_combined_clean, artist = "Tyler, The Creator")
doom_df <- tibble(text = Doom_combined_clean, artist = "MF DOOM")
combined_df <- bind_rows(tyler_df, doom_df)
tidy_lyrics <- combined_df %>%
  unnest_tokens(word, text)

# 1. AFINN Sentiment Analysis
afinn_sentiment <- tidy_lyrics %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(artist) %>%
  summarize(sentiment_score = sum(value))
ggplot(afinn_sentiment, aes(x = artist, y = sentiment_score, fill = artist)) +
  geom_col() +
  labs(title = "AFINN Sentiment Scores by Artist",
       x = "Artist", y = "Sentiment Score") +
  theme_minimal()

# 2. BING Sentiment Analysis
bing_sentiment <- tidy_lyrics %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(artist, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment_ratio = positive / (positive + negative))
ggplot(bing_sentiment, aes(x = artist, y = sentiment_ratio, fill = artist)) +
  geom_col() +
  labs(title = "Positive/Negative Ratio (Bing Lexicon)",
       x = "Artist", y = "Positive Ratio") +
  ylim(0, 1) +
  theme_minimal()

# 3. NRC Sentiment Analysis
nrc_sentiment <- tidy_lyrics %>%
  inner_join(get_sentiments("nrc"), by = "word") %>%
  filter(sentiment %in% c("positive", "negative")) %>%
  count(artist, sentiment)
ggplot(nrc_sentiment, aes(x = sentiment, y = n, fill = artist)) +
  geom_col(position = "dodge") +
  labs(title = "Positive/Negative Sentiment (NRC Lexicon)",
       x = "Sentiment", y = "Count") +
  theme_minimal()

# 4. Emotion Analysis
nrc_emotions <- tidy_lyrics %>%
  inner_join(get_sentiments("nrc"), by = "word") %>%
  filter(!sentiment %in% c("positive", "negative")) %>%
  count(artist, sentiment)
ggplot(nrc_emotions, aes(x = reorder(sentiment, n), y = n, fill = artist)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Emotional Sentiment Distribution (NRC Lexicon)",
       x = "Emotion", y = "Count") +
  theme_minimal()

# 5. Word Clouds
# Tyler Word Cloud
tyler_words <- tidy_lyrics %>%
  filter(artist == "Tyler, The Creator") %>%
  count(word, sort = TRUE)
wordcloud(words = tyler_words$word, freq = tyler_words$n,
          max.words = 100, random.order = FALSE,
          colors = brewer.pal(8, "Dark2"),
          main = "Tyler, The Creator - Most Frequent Words")
# MF DOOM Word Cloud
doom_words <- tidy_lyrics %>%
  filter(artist == "MF DOOM") %>%
  count(word, sort = TRUE)
wordcloud(words = doom_words$word, freq = doom_words$n,
          max.words = 100, random.order = FALSE,
          colors = brewer.pal(8, "Dark2"),
          main = "MF DOOM - Most Frequent Words")
# Tyler Sentiment Cloud
tyler_bing <- tidy_lyrics %>%
  filter(artist == "Tyler, The Creator") %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0)
comparison.cloud(tyler_bing, colors = c("red", "green"),
                 max.words = 100, title.size = 1.5,
                 main = "Tyler, The Creator - Sentiment Word Cloud")
# MF DOOM Sentiment Cloud
doom_bing <- tidy_lyrics %>%
  filter(artist == "MF DOOM") %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0)
comparison.cloud(doom_bing, colors = c("red", "green"),
                 max.words = 100, title.size = 1.5,
                 main = "MF DOOM - Sentiment Word Cloud")
```

# More Negation Analysis:

```{r}
# Negation Analysis --- USING CHATGPT/DEEPSEEK
library(tidytext)
library(dplyr)
library(ggplot2)

# Define negation words
negation_words <- c("not", "no", "never", "without", "don't", "can't", "won't", 
                    "nobody", "nowhere", "isnt", "wasnt", "didnt", "couldnt", 
                    "shouldnt", "wouldnt", "hardly", "barely", "scarcely")
# Process Tyler's negations
tyler_negations <- tibble(text = Tyler_combined_clean) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(word1 %in% negation_words,
         !word2 %in% stop_words$word,
         !is.na(word1)) %>%
  inner_join(get_sentiments("bing"), by = c("word2" = "word")) %>%
  count(word1, word2, sentiment, sort = TRUE) %>%
  group_by(word1) %>%
  slice_max(n, n = 5) %>%
  ungroup()
# Process MF DOOM's negations
doom_negations <- tibble(text = Doom_combined_clean) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(word1 %in% negation_words,
         !word2 %in% stop_words$word,
         !is.na(word1)) %>%
  inner_join(get_sentiments("bing"), by = c("word2" = "word")) %>%
  count(word1, word2, sentiment, sort = TRUE) %>%
  group_by(word1) %>%
  slice_max(n, n = 5) %>%
  ungroup()
# Visualization function for negations
plot_negations <- function(data, artist_name) {
  ggplot(data, aes(x = reorder(paste(word1, word2), n), y = n, fill = sentiment)) +
    geom_col() +
    coord_flip() +
    scale_fill_manual(values = c("positive" = "forestgreen", "negative" = "firebrick")) +
    labs(title = paste("Top Negated Terms in", artist_name, "Lyrics"),
         x = "Negated Phrase", y = "Frequency") +
    theme_minimal() +
    theme(legend.position = "bottom")
}
# Generate plots
plot_negations(tyler_negations, "Tyler, The Creator")
plot_negations(doom_negations, "MF DOOM")
# Combined negation analysis
combined_negations <- bind_rows(
  mutate(tyler_negations, artist = "Tyler"),
  mutate(doom_negations, artist = "DOOM")
) %>%
  group_by(artist, sentiment) %>%
  summarize(total = sum(n), .groups = "drop")
# Sentiment comparison plot
ggplot(combined_negations, aes(x = artist, y = total, fill = sentiment)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c("positive" = "forestgreen", "negative" = "firebrick")) +
  labs(title = "Negated Sentiment Comparison",
       x = "Artist", y = "Count", fill = "Original Sentiment") +
  theme_minimal()
```


# Statistical topic detection modeling (Latennt Dirichlet Allocation)

```{r}
library(topicmodels)
# Tyler_albums
# Doom_albums
Tyler_albums <- Tyler_albums %>%
  mutate(artist = "Tyler")
Doom_albums <- Doom_albums %>%
  mutate(artist = "DOOM")
combined_albums <- bind_rows(Tyler_albums, Doom_albums)

# --- Change to album data instead of artist data --- #

# Clean album text
clean_album_text <- function(txt) {
  txt <- censor_words(txt)
  return(txt)
}
combined_albums <- combined_albums %>%
  rowwise() %>%
  mutate(cleaned_text = clean_album_text(text)) %>%
  ungroup()

# Tokenize
custom_stops <- c("yeah", "uh", "huh")
albums_tidy <- combined_albums %>%
  unnest_tokens(word, cleaned_text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!word %in% custom_stops)
albums_tidy %>%
  count(doc_id, sort = TRUE)

# DTM
albums_dtm <- albums_tidy %>%
  count(doc_id, word, sort = TRUE) %>%
  cast_dtm(document = doc_id, term = word, value = n)
albums_dtm

# Fit LDA
set.seed(123)
albums_lda <- LDA(albums_dtm, k = 4, control = list(seed = 123))
summary(albums_lda)
albums_lda

#.Word-Topic Probs
topic_terms <- tidy(albums_lda, matrix = "beta")
head(topic_terms)
albums_top6 <- topic_terms %>%
  group_by(topic) %>%
  slice_max(beta, n = 6) %>%
  ungroup() %>%
  arrange(topic, -beta)
albums_top6
ggplot(albums_top6, aes(x = beta, reorder_within(term, beta, topic), fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  scale_y_reordered() +
  labs(title = "Top 6 Words per Topic",
       x = "Beta (Probability)",
       y = "Term")

# Gamma probs
album_gamma <- tidy(albums_lda, matrix = "gamma")
head(album_gamma)
album_gamma <- album_gamma %>%
  left_join(combined_albums %>% select(doc_id, artist), by = c("document" = "doc_id"))
album_gamma %>% arrange(document, desc(gamma)) %>% head(10)
top_topic <- album_gamma %>%
  group_by(document, artist) %>%
  slice_max(gamma, n = 1) %>%
  ungroup()
head(top_topic, 5)
# Compare topics between artists
artist_mean_topic <- album_gamma %>%
  group_by(artist, topic) %>%
  summarize(mean_gamma = mean(gamma), .group = "drop")
ggplot(artist_mean_topic, aes(x = factor(topic), y = mean_gamma, fill = artist)) +
  geom_col(position = "dodge") +
  labs(title = "Average Topic Probability by Artist",
       x = "Topic", y = "Mean Gamma")
# Assign Each word to a topic -- similar to HW5 --> then plot/heatmap
# use broom library for augment function -- RDocumentation
library(broom)
words_assigned <- augment(albums_lda, data = albums_dtm)
head(words_assigned)
words_assigned <- words_assigned %>%
  left_join(combined_albums %>% select(doc_id, artist), by = c("document" = "doc_id")) %>%
  group_by(artist, .topic) %>%
  summarise(n = sum(count), .groups = "drop") %>%
  mutate(prop = n/sum(n)) %>%
  ungroup()
ggplot(words_assigned, aes(x = factor(.topic), y = artist, fill = prop)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "salmon", high = "lightblue") +
  labs(title = "Proportion of Words Assigned to Topic by Arttist",
       x = "Topic", y = "Artist", fill = "Prop") +
  theme_minimal()
```


# Random Forest stuff

```{r}
library(randomForest)
library(caTools)
# Get topic-feature df
topic_features <- album_gamma %>%
  pivot_wider(id_cols = c(document, artist),
              names_from = topic,
              values_from = gamma,
              names_prefix = "topic_")
head(topic_features)

# Test/Train split -- HW5 Q2
set.seed(123)
split <- sample.split(topic_features$artist, SplitRatio = 0.7)
train_data <- subset(topic_features, split == TRUE)
test_data <- subset(topic_features, split == FALSE)

nrow(train_data)
nrow(test_data)
table(train_data$artist)
table(test_data$artist)

# Model
train_data$artist <- as.factor(train_data$artist)
test_data$artist <- as.factor(test_data$artist)
artist_rf <- randomForest(artist ~ topic_1 + topic_2 + topic_3 + topic_4, data = train_data)
summary(artist_rf)

# Predictions
artist_preds <- predict(artist_rf, newdata = test_data)
artist_conf_mat <- table(Actual = test_data$artist, Predicted = artist_preds)
artist_conf_mat
artist_rf_accuracy <- sum(diag(artist_conf_mat)) / sum(artist_conf_mat)
cat("Random Forest model accuracy: ", artist_rf_accuracy * 100, "%\n")
```


# Automatic classification using predictive modeling based on text data. (if feasible)
```{r}
# random forest above
```


# Data Clustering (if feasible)

```{r}
# TF-IDF for albums
album_tfidf <- weightTfIdf(albums_dtm, normalize = FALSE)
album_tfidf_mat <- as.matrix(album_tfidf)
album_dist <- dist(album_tfidf_mat)
# Hierarchical clustering
album_hclust <- hclust(album_dist, method = "ward.D2")
plot(album_hclust, hang = -1, cex = 0.8, main = "Hierarchical Clustering of Albums (TF-IDF)")
```


# Visualization of correlations & topics

```{r}
# Done above with corr plots
```

# Comparing Authors, chapters, ... (if feasible)

```{r}
# Sentiment comparison between artists
# Comparison of Negative trends
# Comparison of positive trends
# Correlation Plot for each artist
```


# Document similarities & Text alignment

```{r}
library(proxy)
library(reshape2)
# Heatmap for similarities
album_sim_mat <- as.matrix(simil(album_tfidf_mat, method = "cosine"))
album_sim_df <- melt(album_sim_mat, varnames= c("album_i", "album_j"), value.name = "similarity")
album_names <- rownames(album_tfidf_mat)
album_sim_df$album_i <- factor(album_sim_df$album_i, levels = seq_along(album_names), labels = album_names)
album_sim_df$album_j <- factor(album_sim_df$album_j, levels = seq_along(album_names), labels = album_names)

ggplot(album_sim_df, aes(x = album_i, y = album_j, fill = similarity)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "red", mid = "white", high = "blue", midpoint = 0.5) +
  labs(
    title = "Album–Album Cosine Similarity (TF–IDF)",
    x = "Album",
    y = "Album",
    fill = "Similarity"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# After presentation fixes

```{r}
# Word freq comparison plot
word_freq_df <- data.frame(word = c(names(topfeatures(dfm(Tyler_t), 10)), names(topfeatures(dfm(Doom_t), 10))),
                           freq = c(topfeatures(dfm(Tyler_t), 10), topfeatures(dfm(Doom_t), 10)),
                           artist = rep(c("Tyler", "Doom"), each = 10))

word_freq_df$word <- gsub("niggas", "ni**as", word_freq_df$word)
word_freq_df$word <- gsub("nigga", "ni**a", word_freq_df$word)
word_freq_df$word <- gsub("fuck", "f*ck", word_freq_df$word)
word_freq_df$word <- gsub("shit", "sh*t", word_freq_df$word)

word_freq_df

ggplot(word_freq_df, aes(x = reorder_within(word, freq, artist), y = freq, fill = artist)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~artist, scales = "free") +
  scale_x_discrete(labels = function(x) gsub("___.*", "", x)) +
  labs(title = "Top 10 Words Comparison",
       x = "Word",
       y = "Frequency")

tyler_dtm <- DocumentTermMatrix(Tyler_corp)
tyler_matrix <- as.matrix(tyler_dtm)
tyler_word_freq <- sort(colSums(tyler_matrix), decreasing = TRUE)

names(tyler_word_freq) <- gsub("niggas", "ni**as", names(tyler_word_freq))
names(tyler_word_freq) <- gsub("nigga", "ni**a", names(tyler_word_freq))
names(tyler_word_freq) <- gsub("fuck", "f*ck", names(tyler_word_freq))
names(tyler_word_freq) <- gsub("shit", "sh*t", names(tyler_word_freq))
names(tyler_word_freq) <- gsub("bitch", "b*tch", names(tyler_word_freq))

set.seed(1)
wordcloud(words = names(tyler_word_freq), freq = tyler_word_freq, min.freq = 5, max.words = 60, scale = c(4, 0.75), colors = brewer.pal(8, "Dark2"))

doom_dtm <- DocumentTermMatrix(Doom_corp)
doom_matrix <- as.matrix(doom_dtm)
doom_word_freq <- sort(colSums(doom_matrix), decreasing = TRUE)

names(doom_word_freq) <- gsub("niggas", "ni**as", names(doom_word_freq))
names(doom_word_freq) <- gsub("nigga", "ni**a", names(doom_word_freq))
names(doom_word_freq) <- gsub("fuck", "f*ck", names(doom_word_freq))
names(doom_word_freq) <- gsub("shit", "sh*t", names(doom_word_freq))
names(doom_word_freq) <- gsub("bitch", "b*tch", names(doom_word_freq))

set.seed(1)
wordcloud(words = names(doom_word_freq), freq = doom_word_freq, min.freq = 5, max.words = 60, scale = c(4, 0.75), colors = brewer.pal(8, "Dark2"))

tyler_bigrams <- Tyler_albums %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE)
doom_bigrams <- Doom_albums %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE)


# Bigrams for each artist
tyler_df <- tibble(text = Tyler_combined_clean)
tyler_bigrams <- tyler_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !is.na(word1)) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE) %>%
  top_n(15, n)
doom_df <- tibble(text = Doom_combined_clean)
doom_bigrams <- doom_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !is.na(word1)) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE) %>%
  top_n(15, n)

tyler_bigrams$bigram <- gsub("niggas", "ni**as", tyler_bigrams$bigram)
tyler_bigrams$bigram <- gsub("nigga", "ni**a", tyler_bigrams$bigram)
tyler_bigrams$bigram <- gsub("fuck", "f*ck", tyler_bigrams$bigram)
tyler_bigrams$bigram <- gsub("shit", "sh*t", tyler_bigrams$bigram)
tyler_bigrams$bigram <- gsub("bitch", "b*tch", tyler_bigrams$bigram)

doom_bigrams$bigram <- gsub("niggas", "ni**as", doom_bigrams$bigram)
doom_bigrams$bigram <- gsub("nigga", "ni**a", doom_bigrams$bigram)
doom_bigrams$bigram <- gsub("fuck", "f*ck", doom_bigrams$bigram)
doom_bigrams$bigram <- gsub("shit", "sh*t", doom_bigrams$bigram)
doom_bigrams$bigram <- gsub("bitch", "b*tch", doom_bigrams$bigram)

# Bigram Visualizations
ggplot(tyler_bigrams, aes(x = reorder(bigram, n), y = n)) +
  geom_col(fill = "goldenrod") +
  coord_flip() +
  labs(title = "Top Bigrams in Tyler, The Creator's Lyrics",
       x = "Bigram", y = "Frequency") +
  theme_minimal()
ggplot(doom_bigrams, aes(x = reorder(bigram, n), y = n)) +
  geom_col(fill = "lightblue") +
  coord_flip() +
  labs(title = "Top Bigrams in MF DOOM's Lyrics",
       x = "Bigram", y = "Frequency") +
  theme_minimal()
```

