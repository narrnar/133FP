---
title: "STATS 133 Final Project"
author: "Daren Sathasivam"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Derek - webscrape
##  Tyler, The Creator
```{r}
library(rvest)
library(stringr)
library(httr)
library(dplyr)

# Set artist name and Genius artist page URL
artist_name <- "Tyler, The Creator"
artist_url <- "https://genius.com/artists/Tyler-the-creator"

# Create main artist folder
dir.create(artist_name, showWarnings = FALSE)

# Scrape album URLs from the artist page
artist_page <- tryCatch(read_html(artist_url), error = function(e) NULL)

if (!is.null(artist_page)) {
  album_urls <- artist_page %>%
    html_nodes("a[href*='/albums/Tyler-the-creator/']") %>%
    html_attr("href") %>%
    unique()
  
  # Loop through each album
  for (album_url in album_urls) {
    Sys.sleep(2)  # Delay to prevent rate-limiting
    
    # Extract album name
    album_name <- str_extract(album_url, "/albums/Tyler-the-creator/[^/]+") %>%
      str_replace("/albums/Tyler-the-creator/", "") %>%
      str_replace_all("-", " ") %>%
      str_to_title()
    
    # Create folder for the album
    album_folder <- file.path(artist_name, album_name)
    dir.create(album_folder, showWarnings = FALSE)
    
    # Try to read the album page
    album_page <- tryCatch(read_html(album_url), error = function(e) NULL)
    
    if (!is.null(album_page)) {
      song_urls <- album_page %>%
        html_nodes("a[href*='/Tyler-the-creator-']") %>%
        html_attr("href") %>%
        unique()
      
      # Loop through each song and save lyrics
      for (song_url in song_urls) {
        Sys.sleep(2)  # Delay between requests
        
        # Extract song name
        song_name <- str_extract(song_url, "Tyler-the-creator-[^/]+") %>%
          str_replace("Tyler-the-creator-", "") %>%
          str_replace_all("-", " ") %>%
          str_to_title() %>%
          paste0(".txt")
        
        # Try to read the song page
        song_page <- tryCatch(read_html(song_url), error = function(e) NULL)
        
        if (!is.null(song_page)) {
          # Extract lyrics
          lyrics <- song_page %>%
            html_nodes("div[data-lyrics-container='true']") %>%
            html_text() %>%
            str_squish()
          
          if (length(lyrics) > 0) {  # Check if lyrics were found
            file_path <- file.path(album_folder, song_name)
            
            # Try to write the file
            tryCatch({
              writeLines(lyrics, file_path)
              print(paste("Saved:", file_path))
            }, error = function(e) {
              print(paste("Error writing file:", file_path))
            })
          } else {
            print(paste("No lyrics found for:", song_name))
          }
        } else {
          print(paste("Error loading song page:", song_url))
        }
      }
    } else {
      print(paste("Error loading album page:", album_url))
    }
  }
} else {
  print("Error loading artist page.")
}
```

# Derek - webscrape
## MF DOOM
```{r}
library(rvest)
library(stringr)
library(httr)
library(dplyr)

# Set artist name and Genius artist page URL
artist_name <- "MF DOOM"
artist_url <- "https://genius.com/artists/Mf-doom"

# Create main artist folder
dir.create(artist_name, showWarnings = FALSE)

# Scrape album URLs from the artist page
artist_page <- tryCatch(read_html(artist_url), error = function(e) NULL)

if (!is.null(artist_page)) {
  album_urls <- artist_page %>%
    html_nodes("a[href*='/albums/Mf-doom/']") %>%
    html_attr("href") %>%
    unique()
  
  # Loop through each album
  for (album_url in album_urls) {
    Sys.sleep(2)  # Delay to prevent rate-limiting
    
    # Extract album name
    album_name <- str_extract(album_url, "/albums/Mf-doom/[^/]+") %>%
      str_replace("/albums/Mf-doom/", "") %>%
      str_replace_all("-", " ") %>%
      str_to_title()
    
    # Create folder for the album
    album_folder <- file.path(artist_name, album_name)
    dir.create(album_folder, showWarnings = FALSE)
    
    # Try to read the album page
    album_page <- tryCatch(read_html(album_url), error = function(e) NULL)
    
    if (!is.null(album_page)) {
      song_urls <- album_page %>%
        html_nodes("a[href*='/Mf-doom-']") %>%
        html_attr("href") %>%
        unique()
      
      # Loop through each song and save lyrics
      for (song_url in song_urls) {
        Sys.sleep(2)  # Delay between requests
        
        # Extract song name
        song_name <- str_extract(song_url, "Mf-doom-[^/]+") %>%
          str_replace("Mf-doom-", "") %>%
          str_replace_all("-", " ") %>%
          str_to_title() %>%
          paste0(".txt")
        
        # Try to read the song page
        song_page <- tryCatch(read_html(song_url), error = function(e) NULL)
        
        if (!is.null(song_page)) {
          # Extract lyrics
          lyrics <- song_page %>%
            html_nodes("div[data-lyrics-container='true']") %>%
            html_text() %>%
            str_squish()
          
          if (length(lyrics) > 0) {  # Check if lyrics were found
            file_path <- file.path(album_folder, song_name)
            
            # Try to write the file
            tryCatch({
              writeLines(lyrics, file_path)
              print(paste("Saved:", file_path))
            }, error = function(e) {
              print(paste("Error writing file:", file_path))
            })
          } else {
            print(paste("No lyrics found for:", song_name))
          }
        } else {
          print(paste("Error loading song page:", song_url))
        }
      }
    } else {
      print(paste("Error loading album page:", album_url))
    }
  }
} else {
  print("Error loading artist page.")
}
```



# Extract artist to lyric txt w/ geniusr

```{r}
library(geniusr)
library(dplyr)
library(purrr)
library(stringr)
library(fs)

# Sys.setenv(GENIUS_API_TOKEN = "yxwe42jdT0MvORWPgZ4HdKyNx9axVEQWPCvjkSQ0zjGNUYZ2mGIqBrAuW9CpUAWG")

# --- Client ID --- #
# naXHkMhBTywrOh0ova7Pmh_7tTOcFJ1EpQfKTNvZlyM4wsRPZL7m_--NCwt6an32

# --- Client Secret --- #
# c_1ut-Bgqy7agNISeCTTET6A68C5hxiFiBqB7UFGXQDE4pgmcDewv4pUHIn3cbVWWMHua3lKrh2YLJRtNmxu7Q

# --- Client Access Token --- #
# yxwe42jdT0MvORWPgZ4HdKyNx9axVEQWPCvjkSQ0zjGNUYZ2mGIqBrAuW9CpUAWG
# genius_token(yxwe42jdT0MvORWPgZ4HdKyNx9axVEQWPCvjkSQ0zjGNUYZ2mGIqBrAuW9CpUAWG)
# 
# # Check if token is working:
# search_genius("MF DOOM")
# search_artist("MF DOOM")

scrape_artist_discography <- function(artist_name, artist_num) {
  # Setup directories --- paste ur directory below
  output_dir <- path(
    "/Users/darensivam/Desktop/UCLA/Year 4/Winter/133/Final Project/lyrics", 
    paste0("artist", artist_num)
  )
  dir_create(output_dir)
  
  # 1. Retrieve all possible matches for the given artist_name
  artist_df <- search_artist(artist_name) %>%
    slice(1)  # just take the top row
  # --- Debug: If no match then artist not found --- #
  if (nrow(artist_df) == 0) {
    stop("Artist not found: ", artist_name)
  }
  
  # 2. Get songs with the found artist ID
  songs <- get_artist_songs_df(
    artist_id = artist_df$artist_id,
    include_features = FALSE
  ) %>%
    distinct(song_id, .keep_all = TRUE)

  # --- Debug: If no songs found, you can handle it gracefully --- #
  if (nrow(songs) == 0) {
    message("No songs found for: ", artist_name)
    return(invisible(NULL))
  }
  
  # 3. Group songs by album
  songs %>%
    group_by(
      album_id, 
      album_name = coalesce(album_name, "Non-Album") # stackechange
    ) %>%
    arrange(track_number, .by_group = TRUE) %>%
    group_walk(~ { # stackexhange help
      safe_album_name <- .y$album_name %>%
        str_remove_all("[^\\w ]") %>%
        str_squish()
      # Store album
      album_dir <- path(output_dir, safe_album_name)
      dir_create(album_dir)
      
      # 4. For each song, get lyrics and write to file
      walk2(.x$song_id, seq_along(.x$song_id), ~ {
        song_title <- .x$title %>%
          str_remove_all("[^\\w ]") %>%
          str_squish()
        
        song_file <- path(album_dir, sprintf("%02d_%s.txt", .y, song_title))
        # --- Debug: if song file already exists --- #
        if (!file_exists(song_file)) {
          try({
            lyrics <- get_lyrics_id(.x$song_id)$line %>%
              paste(collapse = "\n") %>%
              str_remove_all("\\[[^\\]]+\\]")  # optional bracket removal
            writeLines(lyrics, song_file)
            Sys.sleep(0.8)  # courtesy delay recommended in 102a by Prof Chen
          }, silent = TRUE)
        }
      })
    })
}


# --- USAGE --- #
# 1) MF DOOM
scrape_artist_discography("MF DOOM", 1)
# 2) Tyler, The Creator
scrape_artist_discography("Tyler, the Creator", 2)
```


# Ethan - Extract txt from discography

```{r}
library(dplyr)
library(stringr)
library(tidytext)
library(readtext)
library(tidyr)
library(quanteda)
library(tm)

Tyler_albums <- readtext("/Users/darensivam/Desktop/UCLA/Year 4/Winter/133/Final Project/lyrics/Tyler, The Creator")
Tyler_combined <- paste(Tyler_albums$text, collapse = " ")

# See what needs to be filtered out
# Tyler_combined

# Some of the sentences include who is speaking in brackets, so we need to filter that out
Tyler_combined_clean <- gsub("\\[.*?\\]", "", Tyler_combined)

# Some of the words did not get spaced properly between verses, ex. arriveIn, so we need to add space for that
Tyler_combined_clean <- gsub("([A-Z])", " \\1", Tyler_combined_clean)

# Random /n in the string, needed to be removed
Tyler_combined_clean <- gsub("\n", " ", Tyler_combined_clean)


Tyler_corp <- Corpus(VectorSource(Tyler_combined_clean))

Tyler_corp <- tm_map(Tyler_corp, content_transformer(tolower)) 
Tyler_corp <- tm_map(Tyler_corp, removePunctuation)  
Tyler_corp <- tm_map(Tyler_corp, removeNumbers) 
Tyler_corp <- tm_map(Tyler_corp, stripWhitespace)
Tyler_corp <- tm_map(Tyler_corp, removeWords, stopwords("en"))  

# inspect(Tyler_corp)

# Inspection looks good, making DTM
DocumentTermMatrix(Tyler_corp)

Tyler_t <- tokens(corpus(Tyler_corp))

# ---------------------------------------------------------------------------------------------------------------


Doom_albums <- readtext("/Users/darensivam/Desktop/UCLA/Year 4/Winter/133/Final Project/lyrics/MF DOOM")
Doom_combined <- paste(Doom_albums$text, collapse = " ")

# Doom_combined

Doom_combined_clean <- gsub("\\[.*?\\]", "", Doom_combined)
Doom_combined_clean <- gsub("([A-Z])", " \\1", Doom_combined_clean)
Doom_combined_clean <- gsub("\n", " ", Doom_combined_clean)

Doom_corp <- Corpus(VectorSource(Doom_combined_clean))

Doom_corp <- tm_map(Doom_corp, content_transformer(tolower)) 
Doom_corp <- tm_map(Doom_corp, removePunctuation)  
Doom_corp <- tm_map(Doom_corp, removeNumbers) 
Doom_corp <- tm_map(Doom_corp, stripWhitespace)
Doom_corp <- tm_map(Doom_corp, removeWords, stopwords("en"))  


# inspect(Doom_corp)

# Inspection looks good, making DTM
DocumentTermMatrix(Doom_corp)

Doom_t <- tokens(corpus(Doom_corp))
```


#  Data cleaned -> proceed with text mining methods/steps

```{r}
# 1. Clean data for specific terms and expressions
head(Doom_combined_clean)
head(Tyler_combined_clean)

# 2. String distances


# 3. Graphical displays of text data

# 4. Natural language processing: stemming, parts-of-speech tagging

# 5. Tokenization and lemmatization

# 6. Creating Corpus

# 7. Creating Document term matrix

# 8. Analysizng Word and Document Frequency: TF-idf

# 9. Relationships Btween Words: N-grams and Correlations

```
# Sentiment Analysis

```{r}
# Use lexicons such as afinn and bing

```


# Statistical topic detection modeling (Latennt Dirichlet Allocation)

```{r}

```


# Automatic classification using predictive modeling based on text data. (if feasible)
```{r}

```


# Data Clustering (if feasible)

```{r}

```


# Visualization of correlations & topics

```{r}

```

# Comparing Authors, chapters, ... (if feasible)

```{r}

```


# Document similarities & Text alignment

```{r}

```

