---
title: "STATS 133 Final Project"
author: "Daren Sathasivam"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Derek - webscrape
##  Tyler, The Creator
```{r}
library(rvest)
library(stringr)
library(httr)
library(dplyr)

# Set artist name and Genius artist page URL
artist_name <- "Tyler, The Creator"
artist_url <- "https://genius.com/artists/Tyler-the-creator"

# Create main artist folder
dir.create(artist_name, showWarnings = FALSE)

# Scrape album URLs from the artist page
artist_page <- tryCatch(read_html(artist_url), error = function(e) NULL)

if (!is.null(artist_page)) {
  album_urls <- artist_page %>%
    html_nodes("a[href*='/albums/Tyler-the-creator/']") %>%
    html_attr("href") %>%
    unique()
  
  # Loop through each album
  for (album_url in album_urls) {
    Sys.sleep(2)  # Delay to prevent rate-limiting
    
    # Extract album name
    album_name <- str_extract(album_url, "/albums/Tyler-the-creator/[^/]+") %>%
      str_replace("/albums/Tyler-the-creator/", "") %>%
      str_replace_all("-", " ") %>%
      str_to_title()
    
    # Create folder for the album
    album_folder <- file.path(artist_name, album_name)
    dir.create(album_folder, showWarnings = FALSE)
    
    # Try to read the album page
    album_page <- tryCatch(read_html(album_url), error = function(e) NULL)
    
    if (!is.null(album_page)) {
      song_urls <- album_page %>%
        html_nodes("a[href*='/Tyler-the-creator-']") %>%
        html_attr("href") %>%
        unique()
      
      # Loop through each song and save lyrics
      for (song_url in song_urls) {
        Sys.sleep(2)  # Delay between requests
        
        # Extract song name
        song_name <- str_extract(song_url, "Tyler-the-creator-[^/]+") %>%
          str_replace("Tyler-the-creator-", "") %>%
          str_replace_all("-", " ") %>%
          str_to_title() %>%
          paste0(".txt")
        
        # Try to read the song page
        song_page <- tryCatch(read_html(song_url), error = function(e) NULL)
        
        if (!is.null(song_page)) {
          # Extract lyrics
          lyrics <- song_page %>%
            html_nodes("div[data-lyrics-container='true']") %>%
            html_text() %>%
            str_squish()
          
          if (length(lyrics) > 0) {  # Check if lyrics were found
            file_path <- file.path(album_folder, song_name)
            
            # Try to write the file
            tryCatch({
              writeLines(lyrics, file_path)
              print(paste("Saved:", file_path))
            }, error = function(e) {
              print(paste("Error writing file:", file_path))
            })
          } else {
            print(paste("No lyrics found for:", song_name))
          }
        } else {
          print(paste("Error loading song page:", song_url))
        }
      }
    } else {
      print(paste("Error loading album page:", album_url))
    }
  }
} else {
  print("Error loading artist page.")
}
```

# Derek - webscrape
## MF DOOM
```{r}
library(rvest)
library(stringr)
library(httr)
library(dplyr)

# Set artist name and Genius artist page URL
artist_name <- "MF DOOM"
artist_url <- "https://genius.com/artists/Mf-doom"

# Create main artist folder
dir.create(artist_name, showWarnings = FALSE)

# Scrape album URLs from the artist page
artist_page <- tryCatch(read_html(artist_url), error = function(e) NULL)

if (!is.null(artist_page)) {
  album_urls <- artist_page %>%
    html_nodes("a[href*='/albums/Mf-doom/']") %>%
    html_attr("href") %>%
    unique()
  
  # Loop through each album
  for (album_url in album_urls) {
    Sys.sleep(2)  # Delay to prevent rate-limiting
    
    # Extract album name
    album_name <- str_extract(album_url, "/albums/Mf-doom/[^/]+") %>%
      str_replace("/albums/Mf-doom/", "") %>%
      str_replace_all("-", " ") %>%
      str_to_title()
    
    # Create folder for the album
    album_folder <- file.path(artist_name, album_name)
    dir.create(album_folder, showWarnings = FALSE)
    
    # Try to read the album page
    album_page <- tryCatch(read_html(album_url), error = function(e) NULL)
    
    if (!is.null(album_page)) {
      song_urls <- album_page %>%
        html_nodes("a[href*='/Mf-doom-']") %>%
        html_attr("href") %>%
        unique()
      
      # Loop through each song and save lyrics
      for (song_url in song_urls) {
        Sys.sleep(2)  # Delay between requests
        
        # Extract song name
        song_name <- str_extract(song_url, "Mf-doom-[^/]+") %>%
          str_replace("Mf-doom-", "") %>%
          str_replace_all("-", " ") %>%
          str_to_title() %>%
          paste0(".txt")
        
        # Try to read the song page
        song_page <- tryCatch(read_html(song_url), error = function(e) NULL)
        
        if (!is.null(song_page)) {
          # Extract lyrics
          lyrics <- song_page %>%
            html_nodes("div[data-lyrics-container='true']") %>%
            html_text() %>%
            str_squish()
          
          if (length(lyrics) > 0) {  # Check if lyrics were found
            file_path <- file.path(album_folder, song_name)
            
            # Try to write the file
            tryCatch({
              writeLines(lyrics, file_path)
              print(paste("Saved:", file_path))
            }, error = function(e) {
              print(paste("Error writing file:", file_path))
            })
          } else {
            print(paste("No lyrics found for:", song_name))
          }
        } else {
          print(paste("Error loading song page:", song_url))
        }
      }
    } else {
      print(paste("Error loading album page:", album_url))
    }
  }
} else {
  print("Error loading artist page.")
}
```



# Extract artist to lyric txt w/ geniusr

```{r}
library(geniusr)
library(dplyr)
library(purrr)
library(stringr)
library(fs)

# Sys.setenv(GENIUS_API_TOKEN = "yxwe42jdT0MvORWPgZ4HdKyNx9axVEQWPCvjkSQ0zjGNUYZ2mGIqBrAuW9CpUAWG")

# --- Client ID --- #
# naXHkMhBTywrOh0ova7Pmh_7tTOcFJ1EpQfKTNvZlyM4wsRPZL7m_--NCwt6an32

# --- Client Secret --- #
# c_1ut-Bgqy7agNISeCTTET6A68C5hxiFiBqB7UFGXQDE4pgmcDewv4pUHIn3cbVWWMHua3lKrh2YLJRtNmxu7Q

# --- Client Access Token --- #
# yxwe42jdT0MvORWPgZ4HdKyNx9axVEQWPCvjkSQ0zjGNUYZ2mGIqBrAuW9CpUAWG
# genius_token(yxwe42jdT0MvORWPgZ4HdKyNx9axVEQWPCvjkSQ0zjGNUYZ2mGIqBrAuW9CpUAWG)
# 
# # Check if token is working:
# search_genius("MF DOOM")
# search_artist("MF DOOM")

scrape_artist_discography <- function(artist_name, artist_num) {
  # Setup directories --- paste ur directory below
  output_dir <- path(
    "/Users/darensivam/Desktop/UCLA/Year 4/Winter/133/Final Project/lyrics", 
    paste0("artist", artist_num)
  )
  dir_create(output_dir)
  
  # 1. Retrieve all possible matches for the given artist_name
  artist_df <- search_artist(artist_name) %>%
    slice(1)  # just take the top row
  # --- Debug: If no match then artist not found --- #
  if (nrow(artist_df) == 0) {
    stop("Artist not found: ", artist_name)
  }
  
  # 2. Get songs with the found artist ID
  songs <- get_artist_songs_df(
    artist_id = artist_df$artist_id,
    include_features = FALSE
  ) %>%
    distinct(song_id, .keep_all = TRUE)

  # --- Debug: If no songs found, you can handle it gracefully --- #
  if (nrow(songs) == 0) {
    message("No songs found for: ", artist_name)
    return(invisible(NULL))
  }
  
  # 3. Group songs by album
  songs %>%
    group_by(
      album_id, 
      album_name = coalesce(album_name, "Non-Album") # stackechange
    ) %>%
    arrange(track_number, .by_group = TRUE) %>%
    group_walk(~ { # stackexhange help
      safe_album_name <- .y$album_name %>%
        str_remove_all("[^\\w ]") %>%
        str_squish()
      # Store album
      album_dir <- path(output_dir, safe_album_name)
      dir_create(album_dir)
      
      # 4. For each song, get lyrics and write to file
      walk2(.x$song_id, seq_along(.x$song_id), ~ {
        song_title <- .x$title %>%
          str_remove_all("[^\\w ]") %>%
          str_squish()
        
        song_file <- path(album_dir, sprintf("%02d_%s.txt", .y, song_title))
        # --- Debug: if song file already exists --- #
        if (!file_exists(song_file)) {
          try({
            lyrics <- get_lyrics_id(.x$song_id)$line %>%
              paste(collapse = "\n") %>%
              str_remove_all("\\[[^\\]]+\\]")  # optional bracket removal
            writeLines(lyrics, song_file)
            Sys.sleep(0.8)  # courtesy delay recommended in 102a by Prof Chen
          }, silent = TRUE)
        }
      })
    })
}


# --- USAGE --- #
# 1) MF DOOM
scrape_artist_discography("MF DOOM", 1)
# 2) Tyler, The Creator
scrape_artist_discography("Tyler, the Creator", 2)
```


# Ethan - Extract txt from discography

```{r}
library(dplyr)
library(stringr)
library(tidytext)
library(readtext)
library(tidyr)
library(quanteda)
library(tm)

censor_words <- function(text) {
  text <- gsub("\\b(n|N)(i|I)(g|G)(g|G)(a|A)\\b", "\\1\\2ooa", text, perl = TRUE)
  text <- gsub("\\b(n|N)(i|I)(g|G)(g|G)(a|A)\\b", "\\1\\2ooas", text, perl = TRUE)
  text <- gsub("\\b(n|N)(i|I)(g|G)(g|G)(e|E)(r|R)\\b", "\\1\\2ooer", text, perl = TRUE)
  text <- gsub("\\b(f|F)(a|A)(g|G)(g|G)(o|O)(t|T)\\b", "\\1\\2ooot", text, perl = TRUE)
  text <- gsub("\\b(n|N)(i|I)(g|G)(g|G)(\\w*)\\b", "\\1\\2oo\\5", text, perl = TRUE)
  text <- gsub("\\b(f|F)(a|A)(g|G)(g|G)(\\w*)\\b", "\\1\\2oo\\5", text, perl = TRUE)
  return(text)
}

remove_custom_punctuation <- content_transformer(function(text) {
  gsub("[[:punct:]&&[^*]]", "", text)
})

Tyler_albums <- readtext("/Users/darensivam/Desktop/UCLA/Year 4/Winter/133/Final Project/lyrics/Tyler, The Creator")
Tyler_combined <- paste(Tyler_albums$text, collapse = " ")

# See what needs to be filtered out
# Tyler_combined

# Some of the sentences include who is speaking in brackets, so we need to filter that out
Tyler_combined_clean <- gsub("\\[.*?\\]", "", Tyler_combined)

# Some of the words did not get spaced properly between verses, ex. arriveIn, so we need to add space for that
Tyler_combined_clean <- gsub("([A-Z])", " \\1", Tyler_combined_clean)

# Random /n in the string, needed to be removed
Tyler_combined_clean <- gsub("\n", " ", Tyler_combined_clean)
Tyler_combined_clean <- censor_words(Tyler_combined_clean)


Tyler_corp <- Corpus(VectorSource(Tyler_combined_clean))

Tyler_corp <- tm_map(Tyler_corp, content_transformer(tolower)) 
Tyler_corp <- tm_map(Tyler_corp, removePunctuation)  
Tyler_corp <- tm_map(Tyler_corp, removeNumbers) 
Tyler_corp <- tm_map(Tyler_corp, stripWhitespace)
Tyler_corp <- tm_map(Tyler_corp, removeWords, stopwords("en"))  

# inspect(Tyler_corp)

# Inspection looks good, making DTM
DocumentTermMatrix(Tyler_corp)

Tyler_t <- tokens(corpus(Tyler_corp))

# ---------------------------------------------------------------------------------------------------------------


Doom_albums <- readtext("/Users/darensivam/Desktop/UCLA/Year 4/Winter/133/Final Project/lyrics/MF DOOM")
Doom_combined <- paste(Doom_albums$text, collapse = " ")

# Doom_combined

Doom_combined_clean <- gsub("\\[.*?\\]", "", Doom_combined)
Doom_combined_clean <- gsub("([A-Z])", " \\1", Doom_combined_clean)
Doom_combined_clean <- gsub("\n", " ", Doom_combined_clean)
Doom_combined_clean <- censor_words(Doom_combined_clean)


Doom_corp <- Corpus(VectorSource(Doom_combined_clean))

Doom_corp <- tm_map(Doom_corp, content_transformer(tolower)) 
Doom_corp <- tm_map(Doom_corp, removePunctuation)  
Doom_corp <- tm_map(Doom_corp, removeNumbers) 
Doom_corp <- tm_map(Doom_corp, stripWhitespace)
Doom_corp <- tm_map(Doom_corp, removeWords, stopwords("en"))  


# inspect(Doom_corp)

# Inspection looks good, making DTM
DocumentTermMatrix(Doom_corp)

Doom_t <- tokens(corpus(Doom_corp))
```

# Outline of things to do:

- 1. Word Analysis: Sentiment Analysis, Word Clouds
- 2. N-grams: Bi-grams by counts, Negation Analysis
- 3. Topic Modeling: LDA, Topic Distribution by Source, then Random Forest


#  Data cleaned -> proceed with text mining methods/steps

```{r}
library(ggplot2)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
# 1. Clean data for specific terms and expressions
# head(Doom_combined_clean, 1)
# head(Tyler_combined_clean, 1)


# 2. String distances
library(stringdist)
tyler_lines <- unlist(strsplit(Tyler_combined_clean, "\\.\\s+"))
doom_lines <- unlist(strsplit(Doom_combined_clean, "\\.\\s+"))
dist_mat <- stringdistmatrix(tyler_lines, doom_lines, method = "jw") # jw for Jaro-Wrinkler dists --- stackexhange ---


# 3. Graphical displays of text data
# Word freq comparison plot
word_freq_df <- data.frame(word = c(names(topfeatures(dfm(Tyler_t), 10)), names(topfeatures(dfm(Doom_t), 10))),
                           freq = c(topfeatures(dfm(Tyler_t), 10), topfeatures(dfm(Doom_t), 10)),
                           artist = rep(c("Tyler", "Doom"), each = 10))
ggplot(word_freq_df, aes(x = reorder(word, freq), y = freq, fill = artist)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~artist, scales = "free") +
  labs(title = "Top 10 Words Comparison")
# Word Clouds -- basic for each artist
tyler_dtm <- DocumentTermMatrix(Tyler_corp)
tyler_matrix <- as.matrix(tyler_dtm)
tyler_word_freq <- sort(colSums(tyler_matrix), decreasing = TRUE)
set.seed(1)
wordcloud(words = names(tyler_word_freq), freq = tyler_word_freq, min.freq = 5, max.words = 60, scale = c(4, 0.75), colors = brewer.pal(8, "Dark2"))
doom_dtm <- DocumentTermMatrix(Doom_corp)
doom_matrix <- as.matrix(doom_dtm)
doom_word_freq <- sort(colSums(doom_matrix), decreasing = TRUE)
set.seed(1)
wordcloud(words = names(doom_word_freq), freq = doom_word_freq, min.freq = 5, max.words = 60, scale = c(4, 0.75), colors = brewer.pal(8, "Dark2"))


# 4. Natural language processing: stemming, parts-of-speech tagging
# Stemming
tyler_stemmed <- tm_map(Tyler_corp, stemDocument)
doom_stemmed <- tm_map(Doom_corp, stemDocument)
# Parts-of-speech tagging
library(udpipe) # stackechange
ud_model <- udpipe_download_model(language = "english")
tyler_pos <- udpipe(Tyler_combined_clean, "english")
doom_pos <- udpipe(Doom_combined_clean, "english")


# 5. Tokenization and lemmatization
# Tokens w/ lemmatization -- different from Tyler_t and Doom_t
tyler_tokens <- tyler_pos %>%
  filter(upos %in% c("NOUN", "VERB", "ADJ")) %>%
  select(token, lemma)
doom_tokens <- doom_pos %>%
  filter(upos %in% c("NOUN", "VERB", "ADJ")) %>%
  select(token, lemma)

# 6. Creating Corpus - done above

# 7. Creating Document term matrix - done above

# 8. Analyzing Word and Document Frequency: TF-idf
tyler_tfidf <- weightTfIdf(DocumentTermMatrix(Tyler_corp))
findMostFreqTerms(tyler_tfidf, n = 15)
doom_tfidf <- weightTfIdf(DocumentTermMatrix(Doom_corp))
findMostFreqTerms(doom_tfidf, n = 15)

# 9. Relationships Btween Words: N-grams and Correlations
# Bigrams
tyler_bigrams <- Tyler_albums %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE)
doom_bigrams <- Doom_albums %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE)
# Word correlations
tyler_tidy <- tidy(DocumentTermMatrix(Tyler_corp))
tyler_word_cor <- tyler_tidy %>%
  pairwise_cor(term, document, sort = TRUE)
doom_tidy <- tidy(DocumentTermMatrix(Doom_corp))
doom_word_cor <- doom_tidy %>%
  pairwise_cor(term, document, sort = TRUE)
# Correlation plots
tyler_dtm_sparse <- removeSparseTerms(tyler_dtm, 0.90)
doom_dtm_sparse <- removeSparseTerms(doom_dtm, 0.90)
tyler_mat_sparse <- as.matrix(tyler_dtm_sparse)
doom_mat_sparse <- as.matrix(doom_dtm_sparse)
dim(tyler_mat_sparse)
dim(doom_mat_sparse)
tyler_sparse_freq <- colSums(tyler_mat_sparse)
doom_sparse_freq <- colSums(doom_mat_sparse)
tyler_sparse_freq <- sort(tyler_sparse_freq, decreasing = TRUE)
doom_sparse_freq <- sort(doom_sparse_freq, decreasing = TRUE)
head(tyler_sparse_freq, 15)
head(doom_sparse_freq, 15)
tyler_sparse_terms <- findFreqTerms(tyler_dtm_sparse, lowfreq = 500)
doom_sparse_terms <- findFreqTerms(doom_dtm_sparse, lowfreq = 125)
# Since corp is under one doc, the corr plot only turns out to be vertical bars because there are no multiple documents.
plot(tyler_dtm_sparse, terms = tyler_sparse_terms, corThreshold = 0.1)
plot(doom_dtm_sparse, terms = doom_sparse_terms, corThreshold = 0.1)
```


```{r}
##### ----- Other stuff ----- #####
library(tokenizers)
# Readability index and average words per sentence
Tyler_qcorp <- corpus(Tyler_combined_clean)
Doom_qcorp <- corpus(Doom_combined_clean)
tyler_readability <- textstat_readability(Tyler_qcorp, measure = c("Flesch", "Flesch.Kincaid"))
doom_readability  <- textstat_readability(Doom_qcorp, measure = c("Flesch", "Flesch.Kincaid"))
tyler_sentences <- tokenize_sentences(Tyler_combined_clean)[[1]]
doom_sentences <- tokenize_sentences(Doom_combined_clean)[[1]]
yler_words_per_sent <- sapply(tokenize_words(tyler_sentences), length)
doom_words_per_sent <- sapply(tokenize_words(doom_sentences), length)
tyler_avg_words <- mean(tyler_words_per_sent[tyler_words_per_sent > 0])
doom_avg_words <- mean(doom_words_per_sent[doom_words_per_sent > 0])

cat("Tyler Readability:\n")
print(tyler_readability)
cat("\nDOOM Readability:\n")
print(doom_readability)
cat("\nAverage Words Per Sentence:\n")
cat("Tyler:", round(tyler_avg_words, 1), "\n")
cat("DOOM:", round(doom_avg_words, 1), "\n")
# Word counts for each artist
tyler_word_count <- ntoken(tyler_qcorp, remove_punct = TRUE)
doom_word_count <- ntoken(doom_qcorp, remove_punct = TRUE)
cat("\nTotal Word Counts:\n")
cat("Tyler:", tyler_word_count, "\n")
cat("DOOM:", doom_word_count, "\n")
# Plot together:
readability_df <- data.frame(
  Artist = c("Tyler", "DOOM"),
  Flesch = c(tyler_readability$Flesch, doom_readability$Flesch),
  FK_Grade = c(tyler_readability$Flesch.Kincaid, doom_readability$Flesch.Kincaid),
  Avg_Words_Per_Sent = c(tyler_avg_words, doom_avg_words),
  Total_Words = c(tyler_word_count, doom_word_count)
)
ggplot(readability_df, aes(x = Artist, y = FK_Grade, fill = Artist)) +
  geom_col() +
  labs(title = "Readability Comparison", y = "Flesch-Kincaid Grade Level")


# Bigrams for each artist
# Top bigrams across both artists
```


# Sentiment Analysis

```{r}
# Use lexicons such as afinn and bing

# Sentiments using ncr
# Positive and negative sentiments for each artist
# Other sentiment trends (9 emotions)
# Word clouds
```


# Statistical topic detection modeling (Latennt Dirichlet Allocation)

```{r}

```


# Automatic classification using predictive modeling based on text data. (if feasible)
```{r}

```


# Data Clustering (if feasible)

```{r}

```


# Visualization of correlations & topics

```{r}

```

# Comparing Authors, chapters, ... (if feasible)

```{r}
# Sentiment comparison between artists
# Comparison of Negative trends
# Comparison of positive trends
# Correlation Plot for each artist
```


# Document similarities & Text alignment

```{r}

```

